[[../index.org][Main Index]]
[[./index.org][Reading List]]

* [[http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf][An Introduction to Statistical Learning]]

1. [X] Introduction
2. [X] Statistical Learning
   1. [X] What Is Statistical Learning?
      1. [X] Why Estimate f?
      2. [X] How Do We Estimate f?
      3. [X] The Trade-Off Between Prediction Accuracy and Model Interpretability
      4. [X] Supervised Versus Unsupervised Learning
      5. [X] Regression Versus Classification Problems
   2. [X] Assessing Model Accuracy
      1. [X] Measuring the Quality of Fit
      2. [X] The Bias-Variance Trade-Off
      3. [X] The Classification Setting
   3. [X] Lab: Introduction to R
      1. [X] Basic Commands
      2. [X] Graphics
      3. [X] Indexing Data
      4. [X] Loading Data
      5. [X] Additional Graphical and Numerical Summaries
   4. [X] Exercises
      + [X] Conceptual
        1. [X]
           + (a) a more flexible method will fit the data closer and since there
             is a large sample size, there is less of a risk of over fitting the
             data.
           + (b) a less flexible method would be preferred since a flexible
             method would over fit the data due to the small number of
             observations.
           + (c) a flexible model would be better in this instance becasue an
             inflexible model will have trouble with non-linear models. It needs
             more degrees of freedom in order to get a small MSE.
           + (d) a more flexible model will be better here since more flexible
             models produce less bias. Since the MSE is the sum of bias and
             variance, and bias is lower with more degrees of freedom, a more
             flexible model will be better.
        2. [X]
           + (a) Regression. inference.
           + (b) Classification. prediction.
           + (c) Regression. prediction.
        3. [X]
           + (a) I sketched this on a piece of paper
           + (b)
             + Bias - as we move towards a more flexible model, bias decreses
               first sharply, then less sharply.
             + Variance - Depending on the model variance can either decrese
               with a more flexible model or increase. This depends on whether
               or not the true model is linear or not.
             + Training Error - more flexible models produce less training error
               since they tend to fit to the data well.
             + Test Error - more flexible models produce more test error since
               they tend to get fit to the training data.
             + Bayes error - Bayes error is sort of a "golden standard" of how
               good the MSE can be.
        4. [X]
           + (a)
             1. sorting fish by types as predicted by size and color. The
                response is the type of fish, and the predictors is size &
                color. For this, the goal is prediction.
             2. image to text classification. The response could be the correct
                letter, and the predictors could be the shape, width, and length
                of the letter stroke. The goal would be predicition
             3. signature verification. The response could be whether or not a
                document was signed by person X. The predictors could be the
                same as 2, but the goal in this case would probably be
                inference.
           + (b)
             1. predicting the length of a cat given parents measurements. The
                response is the length of the cat, and the predictors are the
                parents length. For this the goal could be either predicition or
                inference.
             2. mpg of a car. The reposne is the mpg, and the predictors could
                be number of cylnders, acceleration, top speed, etc. The goal in
                this case would be predicition.
             3. tuition for a university for a given future year. The response
                would be tuition, the predictors could be # of students &
                faculity salaries. This goal would be predicition.
           + (c)
             1. Detecting how a marketing campaign effects different markets.
             2. Detecting how a feature change in an app impacts different
                markets.
             3. Netflix movie recommendations. Recommend movies to groups that
                watch similiar movies .
        5. [X] A flexible approach to regression or classification can have less
           MSE in scenerios where the real model is verny non-linear. A less
           flexible approach tends to be simplier and can be useful for
           inferring how a change in one variable can impact another.
        6. [X] A parametric learning approach assumes a functional form. A
           non-parametric approach does not assume a functional form for f so it
           is more flexible in general ,but also requires a large number of
           observations.
        7. [X]
           1. (a)
              | Obs | X1 | X2 | X3 | Distance | Y     |
              |-----+----+----+----+----------+-------|
              |   1 |  0 |  3 |  0 | 3        | Red   |
              |   2 |  2 |  0 |  0 | 2        | Red   |
              |   3 |  0 |  1 |  3 | sqrt(10) | Red   |
              |   4 |  0 |  1 |  2 | sqrt(5)  | Green |
              |   5 | -1 |  0 |  1 | sqrt(2)  | Green |
              |   6 |  1 |  1 |  1 | sqrt(3)  | Red   |
           2. (b) green. Observarion 5 is the clossent neighbor for K=1.
           3. (c) red. Observatinos 32, 5 and 6 are the closest neighbors for K
              = 3. 2 Is Red, 5 is Green, and 6 is Red which gives us 2/3rds Red.
           4. (d) Small. A small K would be flixible for a non-linear decision
              boundary wheras a large K would try to fit a more linear boundary
              because it takes more points into consideration.
      + [X] Applied
        8. [@8] [X]
           1. (a) okay
           2. (b) okay
           3. (c) okay
        9. [X]
           + (a) Quantitative => (mpg, displacement, horsepower, weight,
             acceleration, year) Qualitative => (origin, name, cylinders)
           + (b) sapply(Auto[, 1:7], range)
           + (c) sapply(Auto[, 1:7], mean)
           + (d) sapply(Auto[, 1:7], sd)
           + (e) okay
           + (f) Yes. weight horsepower and dispacement seem to all have a
             linear relationship with mpg. As each of these increse, mpg
             decreases.
        10. [X]
            + (a) 14 columns and 506 rows. The rows represent the number of
              observations and the columns represent the number of things
              measured or categorized for each observation.
            + (b) okay
            + (c) plot(Boston$age, Boston$crim) Areas with older homes tend to have more crime.
            + (d) Some have particilarly high crime rates. This can be viewed
              with hist(Boston$crim[Boston$crim>1], breaks=25)
            + (e) 35. nrow(subset(Boston, chas == 1))
            + (f) 19.05 median(Boston$ptratio)
            + (g) > t(subset(Boston, medv == min(Boston$medv))) &
              summary(Boston) With these two commands we can find which quartile
              each measurement is in and come to the conclusion that this place
              isn't awesome, but isn't terrible either.
            + (h) nrow(subset(Boston, rm > 7)) = 64. nrow(subset(Boston, rm >
              8)) = 18. These areas have a relatively lower crime rate comparing
              to the range, and a lower lstast also comparing to range.
3. [-] Linear Regression
   1. [X] Simple Linear Regression
      1. [X] Estimating the Coefficients
      2. [X] Assessing the Accuracy of the Coefficient Estimates
      3. [X] Assessing the Accuracy of the Model
   2. [X] Multiple Linear Regression
      1. [X] Estimating the Regression Coefficients
      2. [X] Some Important Questions
   3. [X] Other Considerations in the Regression Model
      1. [X] Qualitative Predictors
      2. [X] Extensions of the Linear Model
      3. [X] Potential Problems
   4. [X] The Marketing Plan
   5. [X] Comparison of Linear Regression with K-Nearest Neighbors
   6. [X] Lab: Linear Regression
      1. [X] Libraries
      2. [X] Simple Linear Regression
      3. [X] Multiple Linear Regression
      4. [X] Interaction Terms
      5. [X] Non-linear Transformations of the Predictors
      6. [X] Qualitative Predictors
      7. [X] Writing Functions
   7. [-] Exercises
      + [-] Conceptual
        1. [X] The null hypotheses associated with table 3.4's p-values are that
           the advertising budgets of TV, radio, and newspaper do not have an
           effect on sales. Based on the very small p values for TV and radio,
           we can reject the null hypotheses for them. We cannot, however reject
           the null hypothesis for newspaper since it has a very small p value.
           We can conclude from this that the newspaper advertising budget does
           not affect sales.
        2. [X] KNN classifier method is (duh) used to solve classification
           problems, whereas the KNN regression method is used to solve
           regression problems. The KNN classifier aims to estimate the
           conditional probability that a measurement belongs to a specific
           class. KNN regression aims to solve regression problems (quantative)
           by identifiying the neighborhood, and then estimating f(x‚ÇÄ) as the
           average of all the training responses in the neighborhood.
        3. [X] The regression line ends up being:
           #+BEGIN_SRC text
             # Regression Line
             y = 50 + 20(GPA) + 0.07(IQ) + 35(Gender) + 0.01(GPA * IQ) - 10(GPA*Gender)

             # For Males
             y = 50 + 20(GPA) + 0.07(IQ) + 0.01(GPA * IQ)

             # For Females
             y = 85 + 10(GPA) + 0.07(IQ) + 0.01(GPA * IQ)
           #+END_SRC
           + [X] (a) iii.
           + [X] (b)
             #+BEGIN_SRC text
               y = 85 + 10(GPA) + 0.07(IQ) + 0.01(GPA * IQ)
               y = 85 + 10(4.0) + 0.07(110) + 0.01(4.0 * 110)
               y = 85 + 40 + 7.7 + 4.4
               y = 137.1
             #+END_SRC
           + [X] (c) False. The size of the cooeficiant isn't important. If we
             want to say that the interaction effect is not statistically
             signficiant we need to test the null hypothesis H‚ÇÄ: ùõΩ‚ÇÑ = 0 and look
             ath the p-value or the F statistic.
        4. [X]
           + [X] (a) Since the true relationship between x & y is linear, it's
             fair to expect that the least squars line would be close to the
             true regression line, so the RSS for the linear regression may be
             lower than for the cubic regression. however, if the test data has
             a small number of observations, or has many outliers, the cubic
             regression RSS may be lower. This is largely dependent on the
             actual data.
           + [X] (b) Again, it's hard to draw a hard conclusion without actually
             being able to see the data, but since the actual fit is linear, we
             wouldn't expect the training data to overfit the linear model. It
             is much more likely, however that the cubic regression would
             overfit to the training data. If this happens, it is likely that
             the cubic regression would have worse RSS.
           + [X] (c) If the relationship is not linear, the cubic model will fit
             better because it is a more flexible model.
           + [X] (d) There is not enough information to know.
        5. [ ] ???
        6. [X] with a bit of algebraic voodoo we can show that avg(y) will equal
           avg(x)
        7. [X] More algebraic voodoo and we're done.
      + [ ] Applied
        8. [@8] [ ]
        9. [ ]
        10. [ ]
        11. [ ]
        12. [ ]
        13. [ ]
        14. [ ]
        15. [ ]

4. Classification 127
   1. [X] An Overview of Classification
   2. [X] Why Not Linear Regression?
   3. [ ] Logistic Regression
   4. [ ] Linear Discriminant Analysis
   5. [ ] A Comparison of Classification Methods
   6. [ ] Lab: Logistic Regression, LDA, QDA, and KNN
   7. [ ] Exercises

5 Resampling Methods 175
5.1 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . 176
5.1.1 The Validation Set Approach . . . . . . . . . . . . . 176
5.1.2 Leave-One-Out Cross-Validation . . . . . . . . . . . 178
5.1.3 k-Fold Cross-Validation . . . . . . . . . . . . . . . . 181
5.1.4 Bias-Variance Trade-Off for k-Fold
Cross-Validation . . . . . . . . . . . . . . . . . . . . 183
5.1.5 Cross-Validation on Classification Problems . . . . . 184
5.2 The Bootstrap . . . . . . . . . . . . . . . . . . . . . . . . . 187
5.3 Lab: Cross-Validation and the Bootstrap . . . . . . . . . . . 190
5.3.1 The Validation Set Approach . . . . . . . . . . . . . 191
5.3.2 Leave-One-Out Cross-Validation . . . . . . . . . . . 192
5.3.3 k-Fold Cross-Validation . . . . . . . . . . . . . . . . 193
5.3.4 The Bootstrap . . . . . . . . . . . . . . . . . . . . . 194
5.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
6 Linear Model Selection and Regularization 203
6.1 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . . 205
6.1.1 Best Subset Selection . . . . . . . . . . . . . . . . . 205
6.1.2 Stepwise Selection . . . . . . . . . . . . . . . . . . . 207
6.1.3 Choosing the Optimal Model . . . . . . . . . . . . . 210
6.2 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . . 214
6.2.1 Ridge Regression . . . . . . . . . . . . . . . . . . . . 215
6.2.2 The Lasso . . . . . . . . . . . . . . . . . . . . . . . . 219
6.2.3 Selecting the Tuning Parameter . . . . . . . . . . . . 227
6.3 Dimension Reduction Methods . . . . . . . . . . . . . . . . 228
6.3.1 Principal Components Regression . . . . . . . . . . . 230
6.3.2 Partial Least Squares . . . . . . . . . . . . . . . . . 237
6.4 Considerations in High Dimensions . . . . . . . . . . . . . . 238
6.4.1 High-Dimensional Data . . . . . . . . . . . . . . . . 238
6.4.2 What Goes Wrong in High Dimensions? . . . . . . . 239
6.4.3 Regression in High Dimensions . . . . . . . . . . . . 241
6.4.4 Interpreting Results in High Dimensions . . . . . . . 243
6.5 Lab 1: Subset Selection Methods . . . . . . . . . . . . . . . 244
6.5.1 Best Subset Selection . . . . . . . . . . . . . . . . . 244
6.5.2 Forward and Backward Stepwise Selection . . . . . . 247
6.5.3 Choosing Among Models Using the Validation
Set Approach and Cross-Validation . . . . . . . . . . 248
xii Contents
6.6 Lab 2: Ridge Regression and the Lasso . . . . . . . . . . . . 251
6.6.1 Ridge Regression . . . . . . . . . . . . . . . . . . . . 251
6.6.2 The Lasso . . . . . . . . . . . . . . . . . . . . . . . . 255
6.7 Lab 3: PCR and PLS Regression . . . . . . . . . . . . . . . 256
6.7.1 Principal Components Regression . . . . . . . . . . . 256
6.7.2 Partial Least Squares . . . . . . . . . . . . . . . . . 258
6.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
7 Moving Beyond Linearity 265
7.1 Polynomial Regression . . . . . . . . . . . . . . . . . . . . . 266
7.2 Step Functions . . . . . . . . . . . . . . . . . . . . . . . . . 268
7.3 Basis Functions . . . . . . . . . . . . . . . . . . . . . . . . . 270
7.4 Regression Splines . . . . . . . . . . . . . . . . . . . . . . . 271
7.4.1 Piecewise Polynomials . . . . . . . . . . . . . . . . . 271
7.4.2 Constraints and Splines . . . . . . . . . . . . . . . . 271
7.4.3 The Spline Basis Representation . . . . . . . . . . . 273
7.4.4 Choosing the Number and Locations
of the Knots . . . . . . . . . . . . . . . . . . . . . . 274
7.4.5 Comparison to Polynomial Regression . . . . . . . . 276
7.5 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . . 277
7.5.1 An Overview of Smoothing Splines . . . . . . . . . . 277
7.5.2 Choosing the Smoothing Parameter Œª . . . . . . . . 278
7.6 Local Regression . . . . . . . . . . . . . . . . . . . . . . . . 280
7.7 Generalized Additive Models . . . . . . . . . . . . . . . . . 282
7.7.1 GAMs for Regression Problems . . . . . . . . . . . . 283
7.7.2 GAMs for Classification Problems . . . . . . . . . . 286
7.8 Lab: Non-linear Modeling . . . . . . . . . . . . . . . . . . . 287
7.8.1 Polynomial Regression and Step Functions . . . . . 288
7.8.2 Splines . . . . . . . . . . . . . . . . . . . . . . . . . . 293
7.8.3 GAMs . . . . . . . . . . . . . . . . . . . . . . . . . . 294
7.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
8 Tree-Based Methods 303
8.1 The Basics of Decision Trees . . . . . . . . . . . . . . . . . 303
8.1.1 Regression Trees . . . . . . . . . . . . . . . . . . . . 304
8.1.2 Classification Trees . . . . . . . . . . . . . . . . . . . 311
8.1.3 Trees Versus Linear Models . . . . . . . . . . . . . . 314
8.1.4 Advantages and Disadvantages of Trees . . . . . . . 315
8.2 Bagging, Random Forests, Boosting . . . . . . . . . . . . . 316
8.2.1 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . 316
8.2.2 Random Forests . . . . . . . . . . . . . . . . . . . . 319
8.2.3 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . 321
8.3 Lab: Decision Trees . . . . . . . . . . . . . . . . . . . . . . . 323
8.3.1 Fitting Classification Trees . . . . . . . . . . . . . . 323
8.3.2 Fitting Regression Trees . . . . . . . . . . . . . . . . 327
Contents xiii
8.3.3 Bagging and Random Forests . . . . . . . . . . . . . 328
8.3.4 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . 330
8.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
9 Support Vector Machines 337
9.1 Maximal Margin Classifier . . . . . . . . . . . . . . . . . . . 338
9.1.1 What Is a Hyperplane? . . . . . . . . . . . . . . . . 338
9.1.2 Classification Using a Separating Hyperplane . . . . 339
9.1.3 The Maximal Margin Classifier . . . . . . . . . . . . 341
9.1.4 Construction of the Maximal Margin Classifier . . . 342
9.1.5 The Non-separable Case . . . . . . . . . . . . . . . . 343
9.2 Support Vector Classifiers . . . . . . . . . . . . . . . . . . . 344
9.2.1 Overview of the Support Vector Classifier . . . . . . 344
9.2.2 Details of the Support Vector Classifier . . . . . . . 345
9.3 Support Vector Machines . . . . . . . . . . . . . . . . . . . 349
9.3.1 Classification with Non-linear Decision
Boundaries . . . . . . . . . . . . . . . . . . . . . . . 349
9.3.2 The Support Vector Machine . . . . . . . . . . . . . 350
9.3.3 An Application to the Heart Disease Data . . . . . . 354
9.4 SVMs with More than Two Classes . . . . . . . . . . . . . . 355
9.4.1 One-Versus-One Classification . . . . . . . . . . . . . 355
9.4.2 One-Versus-All Classification . . . . . . . . . . . . . 356
9.5 Relationship to Logistic Regression . . . . . . . . . . . . . . 356
9.6 Lab: Support Vector Machines . . . . . . . . . . . . . . . . 359
9.6.1 Support Vector Classifier . . . . . . . . . . . . . . . 359
9.6.2 Support Vector Machine . . . . . . . . . . . . . . . . 363
9.6.3 ROC Curves . . . . . . . . . . . . . . . . . . . . . . 365
9.6.4 SVM with Multiple Classes . . . . . . . . . . . . . . 366
9.6.5 Application to Gene Expression Data . . . . . . . . 366
9.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
10 Unsupervised Learning 373
10.1 The Challenge of Unsupervised Learning . . . . . . . . . . . 373
10.2 Principal Components Analysis . . . . . . . . . . . . . . . . 374
10.2.1 What Are Principal Components? . . . . . . . . . . 375
10.2.2 Another Interpretation of Principal Components . . 379
10.2.3 More on PCA . . . . . . . . . . . . . . . . . . . . . . 380
10.2.4 Other Uses for Principal Components . . . . . . . . 385
10.3 Clustering Methods . . . . . . . . . . . . . . . . . . . . . . . 385
10.3.1 K-Means Clustering . . . . . . . . . . . . . . . . . . 386
10.3.2 Hierarchical Clustering . . . . . . . . . . . . . . . . . 390
10.3.3 Practical Issues in Clustering . . . . . . . . . . . . . 399
10.4 Lab 1: Principal Components Analysis . . . . . . . . . . . . 401
xiv Contents
10.5 Lab 2: Clustering . . . . . . . . . . . . . . . . . . . . . . . . 404
10.5.1 K-Means Clustering . . . . . . . . . . . . . . . . . . 404
10.5.2 Hierarchical Clustering . . . . . . . . . . . . . . . . . 406
10.6 Lab 3: NCI60 Data Example . . . . . . . . . . . . . . . . . 407
10.6.1 PCA on the NCI60 Data . . . . . . . . . . . . . . . 408
10.6.2 Clustering the Observations of the NCI60 Data . . . 410
10.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
