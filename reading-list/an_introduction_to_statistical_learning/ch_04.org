[[../index.org][Main Index]]
[[./index.org][Reading List]]
[[../an_introduction_to_statistical_learning.org][Book]]

* Classification
1. [X] An Overview of Classification
2. [X] Why Not Linear Regression?
3. [X] Logistic Regression
4. [X] Linear Discriminant Analysis
5. [X] A Comparison of Classification Methods
6. [X] Lab: Logistic Regression, LDA, QDA, and KNN
7. [-] Exercises
   + [-] Conceptual
     1. [X] nah
     2. [X] also nah
     3. [X] Nah tho
     4. [X]
        + [X] (a) ~9.75%
        + [X] (b) ~9.75%^2 ~= 0.95%
        + [X] (c) ~9.75%^100 ~= 0%
        + [X] (d) the fraction of observations that will be used to make the
          prediction grows exponentially by the number of features. This
          means that there will be less and less observations that can be
          used.
        + [X] (e) apparently the =l = 0.1 for p=1=, =0.1^(1/2) for p=2= and
          =0.1^(1/n) for p=n=
     5. [-]
        + [X] (a) If the Bayes decision boundary is linear, we would expect the
          QDA to perform better on the training set since a higher flexibility
          tends to result in a better fit. However, the LDA should do better
          than QDA on the test set since the QDA would tend to overfit to the
          training set if the real value is linear.
        + [ ] (b) If the Bayes decision boundary is non-linear, we would expect
          the QDA to perform bettor on both the training set and the test set
          due to its higher flexibility. Since the real model is not linear, the
          test data wouldn't be fit well by the model that LDA would make.
        + [X] (c) Generally speaking, QDA will do better if the traning set is
          very large since we don't have to have major concern about the
          variance of the classifier.
        + [X] (d) False, QDA has a higher variance which tends to lead to
          overfit on the test data. This leads to a worse test error rate than a
          lower variance method.
     6. [X]
        + [X] (a) =p(x)=e^(-6+0.05X1+X2)/(1+e^-6+0.05X1+X2)= = 0.3775
        + [X] (b) =p(x)=e^(-6+0.05X1+X2)/(1+e^-6+0.05X1+X2)0.5= solving the
          equation gives us =50 hours=
     7. [ ] I'm not sure how I would know to apply what formula to this, come
        back to later.
     8. [X] Since 1-nearest neighbors has a training error rate of 0% and this
        problem reports the "average" error rate of 18%, there is a test error
        rate of 36% for KNN. We should choose logistic regression in this case
        since it only has 30% error on the test data.
     9. [X]
        + [X] (a) =p(X)/(1-p(X))=0.37= which can be transformed to =p(X)=0.37/(1+0.37)=0.27=
        + [X] (b) this one is a bit simplier since we can just plug it into the
          original formula. We get =0.16/(1-0.16)=0.19= for the odds
   + [-] Applied
     10. [@10] [-]
         + [X] (a) The only pattern seems to be that volume increases over time.
           #+BEGIN_SRC R
             library(ISLR)
             summary(Weekly)
           #+END_SRC
         + [X] (b) Technically, Lag 2 is statistically significant, but it's
           probably just random chance and not actually statistically
           significant.
           #+BEGIN_SRC R
             library(ISLR)
             attach(Weekly)
             fit.glm <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family=binomial)
             summary(fit.glm)
             cor(Weekly[, -9])
             plot(Volume, Year)
           #+END_SRC
         + [X] (c)
           #+BEGIN_SRC R
             library(ISLR)
             attach(Weekly)
             fit.glm <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family=binomial)
             probs <- predict(fit.glm, type = "response")
             ## Uses "rep" or "replicate" to create a vector that has "Down" the length of
             ## probs times.
             pred.glm <- rep("Down", length(probs))
             ## this changes the pred.glm Downs to Up if the matching prob index has a value
             ## of greater than 0.5
             pred.glm[probs > 0.5] <- "Up"
             table(pred.glm, Direction)
           #+END_SRC
           |----------+------+-----|
           | pred.glm | Down |  Up |
           |----------+------+-----|
           | Down     |   54 |  48 |
           | Up       |  430 | 557 |
           |----------+------+-----|
           The confusion matrix tells us that 54 of the predict downs actually
           went down and 557 of the predicted ups went up. In total the model is
           correct =(/ (+ 54 557) (+ 54 430 48 557.0))= => 0.56 of the time.
           This means that the training error rate is =(- 1 0.56)= => 0.44 of
           the time. We could also say that for weeks that the market went up,
           the model is right =(/ 557 (+ 557.0 48))= => 0.92 of the time.
           However, when the market actually went down, the model is only right
           =(/ 54 (+ 54 430.0))= => 0.11 of the time.
         + [ ] (d)
         + [ ] (e)
         + [ ] (f)
         + [ ] (g)
         + [ ] (h)
         + [ ] (i)
     11. [ ]
     12. [ ]
     13. [ ]
     14. [ ]
     15. [ ]
     16. [ ]
     17. [ ]
