[[../index.org][Main Index]]
[[../index.org][Reading List]]
[[../an_introduction_to_statistical_learning.org][Book]]

* Resampling Methods
** Lectures
   + [X] [[https://www.youtube.com/playlist?list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf][Playlist]]
     + [X] [[https://www.youtube.com/watch?v=_2ij6eaaSl0][Estimating Prediction Error and Validation Set Approach]]
     + [X] [[https://www.youtube.com/watch?v=nZAM5OXrktY][K-fold Cross-Validation]]
     + [X] [[https://www.youtube.com/watch?v=S06JpVoNaA0][Cross-Validation: The Right and Wrong Ways]]
     + [X] [[https://www.youtube.com/watch?v=p4BYWX7PTBM][The Bootstrap]]
     + [X] [[https://www.youtube.com/watch?v=BzHz0J9a6k0][More on the Bootstrap]]
     + [X] [[https://www.youtube.com/watch?v=6dSXlqHAoMk][Lab: Cross-Validation]]
     + [X] [[https://www.youtube.com/watch?v=YVSmsWoBKnA][Lab: The Bootstrap]]
** Reading List
1. [X] Cross-Validation
   1. [X] The Validation Set Approach
   2. [X] Leave-One-Out Cross-Validation
   3. [X] k-Fold Cross-Validation
   4. [X] Bias-Variance Trade-Off for k-Fold Cross-Validation
   5. [X] Cross-Validation on Classification Problems
2. [X] The Bootstrap
3. [X] Lab: Cross-Validation and the Bootstrap
   1. [X] The Validation Set Approach
      #+BEGIN_SRC R
        library(ISLR)
        set.seed(1)
        train=sample(392, 196)
        lm.fit=lm(mpg~horsepower, data=Auto, subset=train)
        attach(Auto)
        mean((mpg-predict(lm.fit, Auto))[-train]^2)

        lm.fit2=lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
        mean((mpg-predict(lm.fit2, Auto))[-train]^2)

        lm.fit3=lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
        mean((mpg-predict(lm.fit3, Auto))[-train]^2)

        set.seed(2)
        train=sample(392, 196)
        lm.fit=lm(mpg~horsepower, data=Auto, subset=train)
        attach(Auto)
        mean((mpg-predict(lm.fit, Auto))[-train]^2)

        lm.fit2=lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
        mean((mpg-predict(lm.fit2, Auto))[-train]^2)

        lm.fit3=lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
        mean((mpg-predict(lm.fit3, Auto))[-train]^2)
      #+END_SRC
   2. [X] Leave-One-Out Cross-Validation
      #+BEGIN_SRC R
        library(ISLR)
        attach(Auto)
        set.seed(1)

        glm.fit = glm(mpg~horsepower, data=Auto)
        coef(glm.fit)

        lm.fit = lm(mpg~horsepower, data=Auto)
        coef(glm.fit)

        library(boot)
        glm.fit = glm(mpg~horsepower, data=Auto)
        cv.err = cv.glm(Auto, glm.fit)
        cv.err$delta

        cv.error = rep(0,5)
        for (i in 1:5) {
            glm.fit=glm(mpg~poly(horsepower, i), data=Auto)
            cv.error[i] = cv.glm(Auto, glm.fit)$delta[1]
        }
        cv.error
      #+END_SRC
   3. [X] k-Fold Cross-Validation
      #+BEGIN_SRC R
        library(ISLR)
        attach(Auto)

        set.seed(17)
        cv.error.10 = rep(0, 10)

        for (i in 1:10) {
            glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
            cv.error.10[i] = cv.glm(Auto, glm.fit, K=10)$delta[1]
        }
        cv.error.10
      #+END_SRC
   4. [X] The Bootstrap
      #+BEGIN_SRC R
        alpha.fn = function(data, index) {
            X = data$X[index]
            Y = data$Y[index]
            varX = var(X)
            varY = var(Y)
            covXY = cov(X, Y)
            return ((varY - covXY) / (varX + varY - 2 * covXY))
        }
        alpha.fn(Portfolio, 1:100)

        set.seed(1)
        alpha.fn(Portfolio, sample(100, 100, replace=T))

        boot(Portfolio, alpha.fn, R=1000)

        boot.fn = function(data, index) {
            return (coef(lm(mpg~horsepower, data=data, subset=index)))
        }
        boot.fn(Auto, 1:392)


        set.seed(1)
        boot.fn(Auto, sample(392, 392, replace=T))

        boot(Auto, boot.fn, 1000)

        summary(lm(mpg~horsepower, data=Auto))$coef

        boot.fn = function(data, index) {
            return (coefficients(lm(mpg~horsepower+I(horsepower^2), data=data, subset=index)))
        }
        set.seed(1)
        boot(Auto, boot.fn, 1000)

        summary(lm(mpg~horsepower+I(horsepower^2), data=Auto))$coef
      #+END_SRC
4. [-] Exercises
   + [X] Conceptual
     1. [X] nope
     2. [X]
        + [X] (a) =1-1/n=
        + [X] (b) =1-1/n=
        + [X] (c) Since each observation is independent (duplicates are
          allowed), we just use basic statistics to say that each chance is
          equal and we are doing it n times.
        + [X] (d) ((lambda (n) (- 1 (expt (- 1.0 (/ 1.0 n)) n))) 5) => 0.6723199999999999
        + [X] (e) ((lambda (n) (- 1 (expt (- 1.0 (/ 1.0 n)) n))) 100) => 0.6339676587267709
        + [X] (f) ((lambda (n) (- 1 (expt (- 1.0 (/ 1.0 n)) n))) 10000) => 0.6321389535670295
        + [X] (g)
          #+BEGIN_SRC R
            x <- 1:100000
            plot(x, 1 - (1 - 1/x)^x)
          #+END_SRC
        + [X] (h)
          #+BEGIN_SRC R
            store <- rep(NA, 10000)
            for (i in 1:10000) {
                store[i] = sum(sample(1:100, rep=T)==4)>0
            }
            mean(store)

            # We know from calculus that the limit(1+x/n)^n = e^x which tells us that 1 -
            # 1/e ~= 632 as n goes to infinity.
          #+END_SRC
     3. [X]
        + [X] (a) k-foldcross-validation is implemented by taking the n
          observations and randomly splitting them into k non-overlapping groups
          of length ~= n/k. These groups act as a valadition set, and the
          remainder (length(n-n/k)) acts as a training set. The test error is
          then estimated by averaging the k resulting MSE estimates.
        + [X] (b)
          + [X] i. The validation set approach has two main drowbacks compared
            to k-fold cross-validation. The first is that the validation
            estimate of the test error rate can be highly variable (depending on
            precisely which observations are included in the training set and
            which observations are included in the validation set). The second
            drawback is that only a subset of the observations are used to fit
            the model. Since statistical methods tend to perform worse when
            trained on fewer observatinos, this suggests that the validation set
            error rate may tend to overestimate the test error rate for the
            model fit on the entire data set.
          + [X] ii. The LOOCV cross-validation approach is a special case of
            k-fold cross-validation in which k=n. This approach has two
            drawbacks compared to k-fold cross-validation. First, it requires
            fitting the potentially computationaly expensive model n times
            compared to k-fold cross-validation which requires the model to be
            fitted only k times. Second, the LOOCV cross-validation approach may
            give approximately unbiased estimates of the test error, since each
            training set contains nâˆ’1 observations; however, this approach has
            higher variance than k-fold cross-validation (since we are averaging
            the outputs of n fitted models trained on an almost identical set of
            observations, these outputs are highly correlated, and the mean of
            highly correlated quantities has higher variance than less
            correlated ones). So, there is a bias-variance trade-off associated
            with the choice of k in k-fold cross-validation; typically using k=5
            or k=10 yield test error rate estimates that suffer neither from
            excessively high bias nor from very high variance.
     4. [X] We may estimate the standard deviation of our prediction by using
        the bootstrap method. In this case, rather than obtaining new
        independant data sets from the population and fitting our model on those
        data sets, we obtain repeated random samples from the original data set.
        In this case, we perform sampling with replacement B times and then find
        the corresponding estimates and the standard deviation of those B
        estimates by using equation (5.8).
   + [-] Applied
     5. [@5] [X]
        #+BEGIN_SRC R
          ## a
          library(ISLR)
          attach(Default)
          set.seed(1)
          fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial")
          summary(fit.glm)

          ## b
          #### i
          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          #### ii
          fit.glm <- glm(default ~ income + balance,
                         data = Default,
                         family = "binomial",
                         subset = train)
          summary(fit.glm)
          #### iii
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm <- rep("No", length(probs))
          pred.glm[probs > 0.5] <- "Yes"

          mean(pred.glm != Default[-train, ]$default)

          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm <- rep("No", length(probs))
          pred.glm[probs > 0.5] <- "Yes"
          mean(pred.glm != Default[-train, ]$default)

          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm <- rep("No", length(probs))
          pred.glm[probs > 0.5] <- "Yes"
          mean(pred.glm != Default[-train, ]$default)

          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm <- rep("No", length(probs))
          pred.glm[probs > 0.5] <- "Yes"
          mean(pred.glm != Default[-train, ]$default)

          ## We see that the validation estimate of the test error rate can be variable,
          ## depending on precisely which observations are included in the training set
          ## and which observations are included in the validation set.

          ## d
          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          fit.glm <- glm(default ~ income + balance + student, data = Default, family = "binomial", subset = train)
          pred.glm <- rep("No", length(probs))
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm[probs > 0.5] <- "Yes"
          mean(pred.glm != Default[-train, ]$default)
          ## It doesn't appear that adding the dummy variable leads to a reduction in the
          ## validation set estimate of the test error rate.
        #+END_SRC
     6. [ ]
     7. [ ]
     8. [ ]
     9. [ ]
