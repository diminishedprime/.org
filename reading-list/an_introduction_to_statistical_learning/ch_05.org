[[../index.org][Main Index]]
[[../index.org][Reading List]]
[[../an_introduction_to_statistical_learning.org][Book]]

* Resampling Methods
** Lectures
   + [X] [[https://www.youtube.com/playlist?list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf][Playlist]]
     + [X] [[https://www.youtube.com/watch?v=_2ij6eaaSl0][Estimating Prediction Error and Validation Set Approach]]
     + [X] [[https://www.youtube.com/watch?v=nZAM5OXrktY][K-fold Cross-Validation]]
     + [X] [[https://www.youtube.com/watch?v=S06JpVoNaA0][Cross-Validation: The Right and Wrong Ways]]
     + [X] [[https://www.youtube.com/watch?v=p4BYWX7PTBM][The Bootstrap]]
     + [X] [[https://www.youtube.com/watch?v=BzHz0J9a6k0][More on the Bootstrap]]
     + [X] [[https://www.youtube.com/watch?v=6dSXlqHAoMk][Lab: Cross-Validation]]
     + [X] [[https://www.youtube.com/watch?v=YVSmsWoBKnA][Lab: The Bootstrap]]
** Reading List
1. [X] Cross-Validation
   1. [X] The Validation Set Approach
   2. [X] Leave-One-Out Cross-Validation
   3. [X] k-Fold Cross-Validation
   4. [X] Bias-Variance Trade-Off for k-Fold Cross-Validation
   5. [X] Cross-Validation on Classification Problems
2. [X] The Bootstrap
3. [X] Lab: Cross-Validation and the Bootstrap
   1. [X] The Validation Set Approach
      #+BEGIN_SRC R
        library(ISLR)
        set.seed(1)
        train=sample(392, 196)
        lm.fit=lm(mpg~horsepower, data=Auto, subset=train)
        attach(Auto)
        mean((mpg-predict(lm.fit, Auto))[-train]^2)

        lm.fit2=lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
        mean((mpg-predict(lm.fit2, Auto))[-train]^2)

        lm.fit3=lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
        mean((mpg-predict(lm.fit3, Auto))[-train]^2)

        set.seed(2)
        train=sample(392, 196)
        lm.fit=lm(mpg~horsepower, data=Auto, subset=train)
        attach(Auto)
        mean((mpg-predict(lm.fit, Auto))[-train]^2)

        lm.fit2=lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
        mean((mpg-predict(lm.fit2, Auto))[-train]^2)

        lm.fit3=lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
        mean((mpg-predict(lm.fit3, Auto))[-train]^2)
      #+END_SRC
   2. [X] Leave-One-Out Cross-Validation
      #+BEGIN_SRC R
        library(ISLR)
        attach(Auto)
        set.seed(1)

        glm.fit = glm(mpg~horsepower, data=Auto)
        coef(glm.fit)

        lm.fit = lm(mpg~horsepower, data=Auto)
        coef(glm.fit)

        library(boot)
        glm.fit = glm(mpg~horsepower, data=Auto)
        cv.err = cv.glm(Auto, glm.fit)
        cv.err$delta

        cv.error = rep(0,5)
        for (i in 1:5) {
            glm.fit=glm(mpg~poly(horsepower, i), data=Auto)
            cv.error[i] = cv.glm(Auto, glm.fit)$delta[1]
        }
        cv.error
      #+END_SRC
   3. [X] k-Fold Cross-Validation
      #+BEGIN_SRC R
        library(ISLR)
        attach(Auto)

        set.seed(17)
        cv.error.10 = rep(0, 10)

        for (i in 1:10) {
            glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
            cv.error.10[i] = cv.glm(Auto, glm.fit, K=10)$delta[1]
        }
        cv.error.10
      #+END_SRC
   4. [X] The Bootstrap
      #+BEGIN_SRC R
        alpha.fn = function(data, index) {
            X = data$X[index]
            Y = data$Y[index]
            varX = var(X)
            varY = var(Y)
            covXY = cov(X, Y)
            return ((varY - covXY) / (varX + varY - 2 * covXY))
        }
        alpha.fn(Portfolio, 1:100)

        set.seed(1)
        alpha.fn(Portfolio, sample(100, 100, replace=T))

        boot(Portfolio, alpha.fn, R=1000)

        boot.fn = function(data, index) {
            return (coef(lm(mpg~horsepower, data=data, subset=index)))
        }
        boot.fn(Auto, 1:392)


        set.seed(1)
        boot.fn(Auto, sample(392, 392, replace=T))

        boot(Auto, boot.fn, 1000)

        summary(lm(mpg~horsepower, data=Auto))$coef

        boot.fn = function(data, index) {
            return (coefficients(lm(mpg~horsepower+I(horsepower^2), data=data, subset=index)))
        }
        set.seed(1)
        boot(Auto, boot.fn, 1000)

        summary(lm(mpg~horsepower+I(horsepower^2), data=Auto))$coef
      #+END_SRC
4. [X] Exercises
   + [X] Conceptual
     1. [X] nope
     2. [X]
        + [X] (a) =1-1/n=
        + [X] (b) =1-1/n=
        + [X] (c) Since each observation is independent (duplicates are
          allowed), we just use basic statistics to say that each chance is
          equal and we are doing it n times.
        + [X] (d) ((lambda (n) (- 1 (expt (- 1.0 (/ 1.0 n)) n))) 5) => 0.6723199999999999
        + [X] (e) ((lambda (n) (- 1 (expt (- 1.0 (/ 1.0 n)) n))) 100) => 0.6339676587267709
        + [X] (f) ((lambda (n) (- 1 (expt (- 1.0 (/ 1.0 n)) n))) 10000) => 0.6321389535670295
        + [X] (g)
          #+BEGIN_SRC R
            x <- 1:100000
            plot(x, 1 - (1 - 1/x)^x)
          #+END_SRC
        + [X] (h)
          #+BEGIN_SRC R
            store <- rep(NA, 10000)
            for (i in 1:10000) {
                store[i] = sum(sample(1:100, rep=T)==4)>0
            }
            mean(store)

            # We know from calculus that the limit(1+x/n)^n = e^x which tells us that 1 -
            # 1/e ~= 632 as n goes to infinity.
          #+END_SRC
     3. [X]
        + [X] (a) k-foldcross-validation is implemented by taking the n
          observations and randomly splitting them into k non-overlapping groups
          of length ~= n/k. These groups act as a valadition set, and the
          remainder (length(n-n/k)) acts as a training set. The test error is
          then estimated by averaging the k resulting MSE estimates.
        + [X] (b)
          + [X] i. The validation set approach has two main drowbacks compared
            to k-fold cross-validation. The first is that the validation
            estimate of the test error rate can be highly variable (depending on
            precisely which observations are included in the training set and
            which observations are included in the validation set). The second
            drawback is that only a subset of the observations are used to fit
            the model. Since statistical methods tend to perform worse when
            trained on fewer observatinos, this suggests that the validation set
            error rate may tend to overestimate the test error rate for the
            model fit on the entire data set.
          + [X] ii. The LOOCV cross-validation approach is a special case of
            k-fold cross-validation in which k=n. This approach has two
            drawbacks compared to k-fold cross-validation. First, it requires
            fitting the potentially computationaly expensive model n times
            compared to k-fold cross-validation which requires the model to be
            fitted only k times. Second, the LOOCV cross-validation approach may
            give approximately unbiased estimates of the test error, since each
            training set contains n−1 observations; however, this approach has
            higher variance than k-fold cross-validation (since we are averaging
            the outputs of n fitted models trained on an almost identical set of
            observations, these outputs are highly correlated, and the mean of
            highly correlated quantities has higher variance than less
            correlated ones). So, there is a bias-variance trade-off associated
            with the choice of k in k-fold cross-validation; typically using k=5
            or k=10 yield test error rate estimates that suffer neither from
            excessively high bias nor from very high variance.
     4. [X] We may estimate the standard deviation of our prediction by using
        the bootstrap method. In this case, rather than obtaining new
        independant data sets from the population and fitting our model on those
        data sets, we obtain repeated random samples from the original data set.
        In this case, we perform sampling with replacement B times and then find
        the corresponding estimates and the standard deviation of those B
        estimates by using equation (5.8).
   + [X] Applied
     5. [@5] [X]
        #+BEGIN_SRC R
          ## a
          library(ISLR)
          attach(Default)
          set.seed(1)
          fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial")
          summary(fit.glm)

          ## b
          #### i
          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          #### ii
          fit.glm <- glm(default ~ income + balance,
                         data = Default,
                         family = "binomial",
                         subset = train)
          summary(fit.glm)
          #### iii
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm <- rep("No", length(probs))
          pred.glm[probs > 0.5] <- "Yes"

          mean(pred.glm != Default[-train, ]$default)

          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm <- rep("No", length(probs))
          pred.glm[probs > 0.5] <- "Yes"
          mean(pred.glm != Default[-train, ]$default)

          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm <- rep("No", length(probs))
          pred.glm[probs > 0.5] <- "Yes"
          mean(pred.glm != Default[-train, ]$default)

          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm <- rep("No", length(probs))
          pred.glm[probs > 0.5] <- "Yes"
          mean(pred.glm != Default[-train, ]$default)

          ## We see that the validation estimate of the test error rate can be variable,
          ## depending on precisely which observations are included in the training set
          ## and which observations are included in the validation set.

          ## d
          train <- sample(dim(Default)[1], dim(Default)[1] / 2)
          fit.glm <- glm(default ~ income + balance + student, data = Default, family = "binomial", subset = train)
          pred.glm <- rep("No", length(probs))
          probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
          pred.glm[probs > 0.5] <- "Yes"
          mean(pred.glm != Default[-train, ]$default)
          ## It doesn't appear that adding the dummy variable leads to a reduction in the
          ## validation set estimate of the test error rate.
        #+END_SRC
     6. [X]
        #+BEGIN_SRC R
          ## a
          set.seed(1)
          attach(Default)
          fit.glm <- glm(default ~ income + balance
                      ,  data = Default
                      , family = "binomial"
                        )
          summary(fit.glm)

          ### BO: 4.348e-01
          ### B1: 4.985e-06
          ### B2: 2.274e-04

          ## b
          boot.fn <- function(data, index) {
              fit <- glm(default ~ income + balance
                       , data = data
                       , family = "binomial"
                       , subset = index
                         )
              return (coef(fit))
          }

          ## c
          library(boot)
          boot(Default, boot.fn, 1000)

          ### BO: 4.239273e-01
          ### B1: 4.582525e-06
          ### B2: 2.267955e-04

          ## d

          ### The estimated standard errors obtained by the two methods are pretty close
          ### to one another.
        #+END_SRC
     7. [X]
        #+BEGIN_SRC R
          ## a
          library(ISLR)
          set.seed(1)
          attach(Weekly)
          fit.glm <- glm(Direction ~ Lag1 + Lag2
                       , data = Weekly
                       , family = "binomial"
                         )
          summary(fit.glm)
          ### Deviance Residuals:
          ### Min      1Q  Median      3Q     Max
          ### -1.623  -1.261   1.001   1.083   1.506
          ###
          ### Coefficients:
          ### Estimate Std. Error z value Pr(>|z|)
          ### (Intercept)  0.22122    0.06147   3.599 0.000319 ***
          ### Lag1        -0.03872    0.02622  -1.477 0.139672
          ### Lag2         0.06025    0.02655   2.270 0.023232 *
          ### ---
          ### Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
          ###
          ### (Dispersion parameter for binomial family taken to be 1)
          ###
          ### Null deviance: 1496.2  on 1088  degrees of freedom
          ### Residual deviance: 1488.2  on 1086  degrees of freedom
          ### AIC: 1494.2
          ###
          ### Number of Fisher Scoring iterations: 4


          ## b
          fit.glm.1 <- glm(Direction ~ Lag1 + Lag2
                         , data = Weekly[-1,]
                         , family = "binomial"
                         )
          summary(fit.glm.1)
          ### Deviance Residuals:
          ### Min       1Q   Median       3Q      Max
          ### -1.6258  -1.2617   0.9999   1.0819   1.5071
          ###
          ### Coefficients:
          ### Estimate Std. Error z value Pr(>|z|)
          ### (Intercept)  0.22324    0.06150   3.630 0.000283 ***
          ### Lag1        -0.03843    0.02622  -1.466 0.142683
          ### Lag2         0.06085    0.02656   2.291 0.021971 *
          ### ---
          ### Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
          ###
          ### (Dispersion parameter for binomial family taken to be 1)
          ###
          ### Null deviance: 1494.6  on 1087  degrees of freedom
          ### Residual deviance: 1486.5  on 1085  degrees of freedom
          ### AIC: 1492.5
          ###
          ### Number of Fisher Scoring iterations: 4


          ## c
          predict.glm(fit.glm.1
                    , Weekly[1, ]
                    , type = "response"
                      ) > 0.5
          ### TRUE

          ### We may conclude that the prediction for the first observation is "Up". This
          ### observation was not correctly classified as the true direction is "Down"

          ## d
          error <- rep(0, dim(Weekly)[1])
          for (i in 1:dim(Weekly)[1]) {
              fit.glm <- glm(Direction ~ Lag1 + Lag2
                           , data = Weekly[-i, ]
                           , family = "binomial")
              pred.up <- predict.glm(fit.glm
                                   , Weekly[i, ]
                                   , type = "response") > 0.5
              true.up <- Weekly[i, ]$Direction == "Up"
              if (pred.up != true.up) {
                  error[i] <- 1
              }
          }
          error

          ## e
          mean(error)

          ### 0.4499541

          ### The LOOCV estimate for the test error rate is 44.99%
        #+END_SRC
     8. [X]
        #+BEGIN_SRC R
          ## a
          set.seed(1)
          y <- rnorm(100)
          x <- rnorm(100)
          y <- x - 2*x^2 + rnorm(100)

          ### n is 100
          ### p is 2

          ## b
          plot(x, y)

          ## The data suggests a curved (looks a lot like a parabola) relationship.

          ## c
          library(boot)
          set.seed(1)
          Data <- data.frame(x, y)
          fit.glm.1 <- glm(y ~ x)
          cv.glm(Data, fit.glm.1)$delta[1]

          fit.glm.2 <- glm(y ~ poly(x, 2))
          cv.glm(Data, fit.glm.2)$delta[1]

          fit.glm.3 <- glm(y ~ poly(x, 3))
          cv.glm(Data, fit.glm.3)$delta[1]

          fit.glm.4 <- glm(y ~ poly(x, 4))
          cv.glm(Data, fit.glm.4)$delta[1]

          ## d
          library(boot)
          set.seed(2)
          Data <- data.frame(x, y)
          fit.glm.1 <- glm(y ~ x)
          cv.glm(Data, fit.glm.1)$delta[1]

          fit.glm.2 <- glm(y ~ poly(x, 2))
          cv.glm(Data, fit.glm.2)$delta[1]

          fit.glm.3 <- glm(y ~ poly(x, 3))
          cv.glm(Data, fit.glm.3)$delta[1]

          fit.glm.4 <- glm(y ~ poly(x, 4))
          cv.glm(Data, fit.glm.4)$delta[1]

          ### The results were identical in c and d because LOOCV evaluates n folds of a
          ### single observation.

          ## e
          ### We see that the LOOCV estimate for the test MSE is minimum for fit.glm.2.
          ### This is not surprising for a bunch of reasons, not least of all being the
          ### picture we saw in b show a quadratic relationship.

          ## f
          summary(fit.glm.4)

          ### Deviance Residuals:
          ### Min       1Q   Median       3Q      Max
          ### -2.8914  -0.5244   0.0749   0.5932   2.7796
          ###
          ### Coefficients:
          ### Estimate Std. Error t value Pr(>|t|)
          ### (Intercept)  -1.8277     0.1041 -17.549   <2e-16 ***
          ### poly(x, 4)1   2.3164     1.0415   2.224   0.0285 *
          ### poly(x, 4)2 -21.0586     1.0415 -20.220   <2e-16 ***
          ### poly(x, 4)3  -0.3048     1.0415  -0.293   0.7704
          ### poly(x, 4)4  -0.4926     1.0415  -0.473   0.6373
          ### ---
          ### Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
          ###
          ### (Dispersion parameter for gaussian family taken to be 1.084654)
          ###
          ### Null deviance: 552.21  on 99  degrees of freedom
          ### Residual deviance: 103.04  on 95  degrees of freedom
          ### AIC: 298.78
          ###
          ### Number of Fisher Scoring iterations: 2

          ### These results show that the linear and quadratic terms are statistically
          ### significant while the cubic and 4th degree terms are not. This does agree
          ### with our cross-validation results which had a minimum for the quadratic
          ### model.
        #+END_SRC
     9. [X]
        #+BEGIN_SRC R
          ## a
          library(MASS)
          attach(Boston)
          mu.hat <- mean(medv)
          mu.hat ## => 22.53281

          ## b
          se.hat <- sd(medv) / sqrt(length(medv))
          se.hat ## => 0.4088611

          ## c
          set.seed(1)
          boot.fn <- function(data, index) {
              mu <- mean(data[index])
              return (mu)
          }
          boot(medv, boot.fn, 1000)
          ### Bootstrap Statistics :
          ### original      bias         std. error
          ### t1* 22.53281  0.008517589  0.4119374

          ### The bootstrap estimated standard error of mu.hat of 0.4119 is very close the
          ### the estimate of 0.4088 found in b.

          ## d
          t.test(medv)

          ### One Sample t-test
          ###
          ### data:  medv
          ### t = 55.111, df = 505, p-value < 2.2e-16
          ### alternative hypothesis: true mean is not equal to 0
          ### 95 percent confidence interval:
          ### 21.72953 23.33608
          ### sample estimates:
          ### mean of x
          ### 22.53281

          CI.mu.hat <- c(22.53 -2 * 0.4119, 22.53 + 2 * 0.4119)
          CI.mu.hat ## => 21.7062 23.3538

          ### The bootstrap confidence interval is very close to the one provided by the
          ### t.test() function.

          ## e
          med.hat <- median(medv)
          med.hat ## => 21.2

          ## f
          boot.fn <- function(data, index) {
              mu <- median(data[index])
              return (mu)
          }
          boot(medv, boot.fn, 1000)
          ### Bootstrap Statistics :
          ### original     bias      std. error
          ### t1*     21.2 -0.0098   0.3874004

          ### We get an estimated median value fo 21.2 which is equal to the value
          ### obtained in e, with a standard error of 0.3874 which is relatively small
          ### compared to the median value.

          ## g
          percent.hat <- quantile(medv, c(0.1))
          percent.hat ## => 12.75

          boot.fn <- function(data, index) {
              mu <- quantile(data[index], c(0.1))
              return (mu)
          }
          boot(medv, boot.fn, 1000)
          ### Bootstrap Statistics :
          ### original      bias      std. error
          ### t1*    12.75  0.00515   0.5113487

          ## We get an estimated tenth percentile value of 12.75 which is again equal to
          ## the value obtained in g, whith a standard error of 0.5113 which is relatively
          ## small compared to percentile value.
        #+END_SRC
